# =====================================================
# Global Experiment Settings
# =====================================================

# General Configuration
project_name: "cryptovision"    # Project name (used for wandb and directory organization)
experiment_name: "cv_model_001" # Name of the experiment run or None
tags: []                        # List of tags for experiment categorization
image_size: 299                 # Target image size (both width and height)
verbose: true                   # Verbosity flag for logging output
version: "auto"                 # Experiment version identifier (auto uses current timestamp)
save: false                     # Flag to enable saving of models and artifacts
seed: 42                        # Random seed for reproducibility

# Warnings and Logging
suppress_warnings: false        # Suppress TensorFlow and other system warnings

# =====================================================
# Data Configuration
# =====================================================

data_path: "/Data/Sources"      # Root directory for image data
test_size: 0.15                 # Proportion of data to reserve for testing
validation_size: 0.15           # Proportion of data to reserve for validation
batch_size: 32                  # Batch size for training and evaluation
samples_threshold: 90           # Minimum number of samples per class to include

# =====================================================
# Model Architecture Settings
# =====================================================

pretrain: "rn50v2"              # Pre-trained model backbone (options: rn50v2, rn152v2, efv2b0, efv2b1, vgg16)
architecture: "std"             # Architectural variant; options: gated, concat, std, att
se_block: true                  # SE Block; options: true or false
shared_dropout: 0.3             # Dropout rate for shared (common) layers
features_dropout: 0.3           # Dropout rate for feature extraction layers
shared_layer_neurons: 2048      # Number of neurons in the shared layers (e.g., 256, 512, 1024, 2048)
pooling_type: "max"             # Pooling type; options: "avg" or "max"

# =====================================================
# Compilation Settings
# =====================================================

lr: 0.0005                      # Learning rate for initial training
loss_type: "tfcl"               # Loss function to use
metrics: ["accuracy", "Precision", "Recall", "AUC"]  # Evaluation metrics
loss_weights: [1.0, 1.0, 1.0]   # Loss weights for each output (family, genus, species)

# =====================================================
# Training Settings
# =====================================================

epochs: 15                  # Number of training epochs

# Early Stopping Configuration
early_stop:
  patience: 3               # Number of epochs with no improvement before stopping
  monitor: "val_loss"       # Metric to monitor for early stopping
  best_weights: true        # Whether to restore the best model weights after stopping

# Learning Rate Reduction Settings
reduce_lr:
  monitor: "val_loss"       # Metric to monitor for reducing learning rate
  factor: 0.2               # Factor by which the learning rate will be reduced
  patience: 2               # Patience (in epochs) before reducing the learning rate
  min: 0.000001             # Minimum learning rate allowed
  scheduler: true           # Whether to use a learning rate scheduler
  scheduler_factor: 0.2     # Factor by which the learning rate will be reduced
  scheduler_epochs: [6, 11] # Epochs at which to reduce the learning rate

# Checkpoint Settings
checkpoint:
  monitor: "val_loss"       # Metric to monitor for saving model checkpoints
  mode: "min"               # Mode for checkpointing ('min' for loss, 'max' for accuracy, etc.)
  save_best_only: true      # Save only the model with the best performance
  weights_only: true     # Save only the model weights during fine-tuning

# =====================================================
# Fine-Tuning Settings
# =====================================================
# These settings are specifically applied during the fine-tuning phase.

finetune:
  weights_path: "/Users/leonardo/Documents/Projects/cryptovision/models/ArchCompare/2504.23.1737/final.weights.h5"
  epochs: 10               # Number of epochs dedicated to fine-tuning
  unfreeze_layers: 75      # Number of layers in the pre-trained module to keep trainable during fine-tuning
  lr: 0.00001              # Learning rate for the fine-tuning phase
  loss_type: "tfcl"        # Loss function for fine-tuning
  metrics: ["accuracy", "Precision", "Recall", "AUC"]  # Metrics to evaluate during fine-tuning
  loss_weights: [1.0, 1.0, 1.0]  # Loss weights during fine-tuning
  early_stop:
    patience: 4            # Patience for early stopping during fine-tuning
    monitor: "val_loss"    # Metric to monitor for early stopping
    best_weights: true     # Restore best weights after early stopping
  reduce_lr:
    monitor: "val_loss"    # Metric to monitor for reducing learning rate during fine-tuning
    factor: 0.2            # Factor for reducing the learning rate during fine-tuning
    patience: 2            # Patience (in epochs) before reducing the learning rate
    min: 0.0000001         # Minimum learning rate allowed during fine-tuning
  checkpoint:
    monitor: "val_loss"    # Metric to monitor for saving model checkpoints during fine-tuning
    mode: "min"            # Mode for checkpointing during fine-tuning
    save_best_only: true   # Save only the best model during fine-tuning
    weights_only: true     # Save only the model weights during fine-tuning

# =====================================================
# Grid Search Configuration
# =====================================================

grid_search:
  architecture: ['gated', 'concat', 'std', 'att']
  loss_type: ['cfc', 'tfcl']
  seed: [42, 1, 17]